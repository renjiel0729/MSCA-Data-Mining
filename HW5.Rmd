---
title: "HW5-1"
author: "Renjie Liu"
date: "3/2/2021"
output: html_document
---
#### 1.Load in Anil’s clustreg and clustreg.predict functions; Dataset is: GermanCreditData.
#### 2. Amount will be the dependent variable. 3.Use the other 6 numeric variables as independent variables. If you use caret package data, this will be columns 1 and 3 through 7 (typo is assignment). Align your data frame for training and test so that amount in the first column and the following 6 are the other 6 numeric variables.
```{r}
data <- read.csv('GermanCredit.csv')
set.seed(123)
data <- data[,-1]
data1 <- data[,c(2,1,3:7)]
train_index<- sample(nrow(data1), size = 0.7*nrow(data1))
train <- data1[train_index,]
test <- data1[-train_index,]

test <- as.data.frame(test)
```

```{r}
clustreg=function(dat,k,tries,sed,niter)
{

set.seed(sed)
dat=as.data.frame(dat)
rsq=rep(NA,niter)
res=list()
rsq.best=0
    for(l in 1:tries) 
    {

	c = sample(1:k,nrow(dat),replace=TRUE)
	yhat=rep(NA,nrow(dat))
	for(i in 1:niter) 
	{		
		resid=pred=matrix(0,nrow(dat),k)
		for(j in 1:k)
		{	
			pred[,j]=predict(glm(dat[c==j,],family="gaussian"),newdata=dat)		
			resid[,j] = (pred[,j]-dat[,1])^2
		}

	c = apply(resid,1,which.min)
	for(m in 1:nrow(dat)) {yhat[m]=pred[m,c[m]]}
	rsq[i] = cor(dat[,1],yhat)^2	
	}
	
	if(rsq[niter] > rsq.best) 
		{	
		rsq.best=rsq[niter]
		l.best=l
            	c.best=c
		yhat.best=yhat
		}
    }

    res=list("Complete")
    for(i in k:1) {res=list(summary(lm(dat[c.best==i,])),res)}
	
    return(list(data=dat,nclust=k,tries=tries,seed=sed,rsq.best=rsq.best,number.loops=niter, Best.try=l.best,cluster=c.best,results=res))
}
```

#### 4. Perform clusters wise regression for 1, 2, and 3 clusters. Arguments for clustreg formula are: dat, k, tries, seed, niter
```{r}
model1 <- clustreg(train,1,sed = 1234, tries = 1, niter = 1)


model2 <- clustreg(train,2,sed = 1234, tries = 2, niter = 10)


model3 <- clustreg(train,3,sed = 1234, tries = 2, niter = 10)
```
#### Explore the attributes and find the best R squared for each cluster solution. Plot this r squared as a function of number of clusters
```{r}
model1$results
model2$results
model3$results


r2 <- c(model1$rsq.best,model2$rsq.best,model3$rsq.best)
x1 <- c(1,2,3)
plot(x = x1, y = r2, xlab = 'Number of Clusters',ylab = 'Train R-squared')
```

#### 6. Perform holdout on each of the 3 cluster models using the clustreg.predict function. Arguments are the appropriate train model and holdout data
```{r}
clustreg.predict=function(results,newdat){

	yhat=rep(NA,nrow(newdat))
	resid=pred=matrix(0,nrow(newdat),length(table(results$cluster)))
		
		for(j in 1:length(table(results$cluster))){			
			pred[,j]=predict(glm(results$data[results$cluster==j,],family="gaussian"),newdata=newdat)		
			resid[,j] = (pred[,j]-newdat[,1])^2
		}

	c = apply(resid,1,which.min)
	for(m in 1:nrow(newdat)) {yhat[m]=pred[m,c[m]]}
	rsq = cor(newdat[,1],yhat)^2	

return(list(results=results,newdata=newdat,cluster=c,yhat=yhat,rsq=rsq))

}

```

```{r}
pred1 <- clustreg.predict(model1,test)


pred2 <- clustreg.predict(model2,test)


pred3 <- clustreg.predict(model3,test)


```
#### 7. Find the best r squared from each of the holdout models. Show a table of R squared from train, r squared from holdout, and percentage decrease from train to holdout R squared: (Train R square – Holdout R square)/ Train R square.
```{r}
r2 <- c(pred1$rsq,pred2$rsq,pred3$rsq)
x1 <- c(1,2,3)
plot(x = x1, y = r2, xlab = 'Number of Clusters',ylab = 'Test R-squared')
```
```{r}
testrsq <- c(pred1$rsq,pred2$rsq,pred3$rsq)
trainrsq <- c(model1$rsq.best,model2$rsq.best,model3$rsq.best)
table <- data.frame(trainrsq,testrsq)
table$rsq_decrease_percentage <- (table$trainrsq-table$testrsq)/table$trainrsq*100
table
```

#### 8. Comment on your solution. From training plot of R squared, which solution seems best? How did each solution perform in holdout? Are you able to interpret the results? Can you tell what types of clusters have been formed?
```{r}
pred1$results[9]
pred2$results[9]
pred3$results[9]

# From the training plot of R squared, k = 3 clusters is the best solution. When using the test sets, k = 3 still has the best r squared. The solution in the holdout are ok. For k = 1, the r squared actually increased 12.7%. For k =2, the r squared decreased 6.4%. For k = 3, the test r squared decreased only 0.56%, so it is very stable. When k = 1, only the intercept, duration, installmentratepercentage, and age are siginificant factors. When k =2, The first cluster has significant factors: intercept, duration, installmentratepercentage, age, and numberpeople maintenance.The second cluster has different significant factors: Intercept, duration, installmentratepercentage, and numberexistingcredits. When k = 3, The first cluster has siginificant factors: Interceopt, duration, installmentratepercentage, and age, the same factors as the cluster in k = 1. The second cluster in k = 3 has significant factors: duration, residenceduration, numberexistingcredits, and numberpeoplemaintennace. This is the first time residenceduration becomes significant. The third cluster in k = 3 only has intercept and duration as significant factors. 
```















